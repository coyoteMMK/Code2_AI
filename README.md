# CODE-2 Translator (T5) â€” NL â†’ Ensamblado CODE-2

Este repositorio contiene un sistema completo para **traducir instrucciones en lenguaje natural** a **ensamblado CODE-2** mediante un modelo **T5 (Seq2Seq)** entrenado con un **dataset sintÃ©tico** generado automÃ¡ticamente.

Incluye:
- GeneraciÃ³n de dataset (NL â†’ CODE-2) en JSON
- Entrenamiento (Full fine-tuning)
- EvaluaciÃ³n con mÃ©tricas (Exact Match, BLEU, ROUGE-L y tiempo de inferencia)
- CuantizaciÃ³n (PTQ dinÃ¡mico INT8 en PyTorch y ONNX INT8 dinÃ¡mico)
- Despliegue local (CLI y Gradio)
- Despliegue en Hugging Face Spaces (Gradio)

---

## âœ¨ CaracterÃ­sticas principales

- TraducciÃ³n de mÃºltiples lÃ­neas: la entrada puede contener varias instrucciones separadas por saltos de lÃ­nea.
- Manejo de errores: el generador incorpora ejemplos con registros/direcciones invÃ¡lidas para mejorar robustez.
- NormalizaciÃ³n de salida: elimina tokens especiales, espacios sobrantes y preserva los saltos de lÃ­nea reales.
- OptimizaciÃ³n para inferencia: opciÃ³n ONNX INT8 dinÃ¡mico con mejoras reales de tiempo en CPU.

---

## ðŸ“ Estructura del repositorio

```text
CODE2-T5/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.json
â”‚   â”œâ”€â”€ valid.json
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ full_fp32/
â”‚   â””â”€â”€ onnx_int8_dynamic/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data_generation.py
â”‚   â”œâ”€â”€ training.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ export_onnx_quantize.py
â”‚   â””â”€â”€ compare_models.py
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ cli.py
â”‚   â””â”€â”€ app_gradio.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## âš™ï¸ InstalaciÃ³n

Recomendado: entorno virtual.

```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

Instalar dependencias:

```bash
pip install -r requirements.txt
```

> Nota: si se usa `transformers`, es importante mantener `huggingface-hub < 1.0` para compatibilidad.

---

## ðŸš€ Uso rÃ¡pido (CLI)

Ejecuta el traductor desde consola y elige el modelo (FP32 u ONNX INT8):

```bash
python deployment/cli.py
```

Ejemplo de entrada:

```text
Suma r1 y r2 y guarda en r3
Guarda r3 en la direcciÃ³n 0345
```

Salida esperada:

```text
ADDS r3,r1,r2
ST [rD+H'45'],r3 ; rD = 0300
```

---

## ðŸŒ Demo web (Gradio)

### Local

```bash
python deployment/app_gradio.py
```

La interfaz permite:

* Cambiar entre modelo **FP32** y **ONNX INT8**
* Ajustar `num_beams` (beam search)
* Medir tiempo de inferencia en cada ejecuciÃ³n

### Hugging Face Spaces

Este mismo archivo es compatible con Spaces (modo CPU).
Solo es necesario subirlo como `app.py` junto con `requirements.txt` y los directorios de modelo.

---

## ðŸ§ª EvaluaciÃ³n (mÃ©tricas)

EvalÃºa un modelo usando `valid.json`:

```bash
python scripts/evaluate.py --model_path models/full_fp32 --valid_path data/valid.json
```

MÃ©tricas calculadas:

* Exact Match (con normalizaciÃ³n de saltos y tokens especiales)
* BLEU
* ROUGE-L
* Tiempo medio de inferencia
* Tokens medios generados

---

## ðŸ§  Entrenamiento (Full fine-tuning)

Entrena T5 con el dataset JSON generado:

```bash
python scripts/training.py --train data/train.json --valid data/valid.json --save_dir models/full_fp32
```

El entrenamiento:

* Tokeniza entrada/salida con padding y truncation
* Ajusta `max_length` en funciÃ³n del dataset
* Guarda modelo y tokenizer al final

---

## ðŸ§° GeneraciÃ³n de datos (dataset sintÃ©tico)

Genera un dataset NL â†’ CODE-2 en formato JSON:

```bash
python scripts/data_generation.py
```

El generador:

* Produce instrucciones vÃ¡lidas y casos con error (â‰ˆ10%)
* Evita duplicados
* Construye bloques multilÃ­nea (hasta 11 instrucciones)
* Mantiene saltos de lÃ­nea como `\n` reales

---

## ðŸ§® CuantizaciÃ³n (PTQ INT8)

### 1) PTQ dinÃ¡mico INT8 (PyTorch)

Se aplica `dynamic quantization` sobre capas `nn.Linear` en CPU.

```bash
python scripts/ptq_dynamic_int8.py --model_path models/full_fp32 --out_dir models/ptq_int8_dynamic
```

### 2) ExportaciÃ³n a ONNX + cuantizaciÃ³n INT8 dinÃ¡mica

```bash
python scripts/export_onnx_quantize.py --model_path models/full_fp32 --out_dir models/onnx_int8_dynamic
```

---

## âž• CÃ³mo aÃ±adir nuevas instrucciones (extender CODE-2)

1. Edita `scripts/data_generation.py`:

   * AÃ±ade una plantilla NL nueva
   * AÃ±ade el formato CODE-2 correspondiente
2. Genera dataset:

   ```bash
   python scripts/data_generation.py
   ```
3. Entrena o continÃºa entrenamiento:

   ```bash
   python scripts/training.py --train data/train.json --valid data/valid.json --save_dir models/full_fp32
   ```

---

## ðŸ“Œ Reproducibilidad

* Se fija `seed` en entrenamiento/evaluaciÃ³n.
* Se mantiene un pipeline completo: generaciÃ³n â†’ entrenamiento â†’ evaluaciÃ³n â†’ despliegue.
* Los modelos se guardan junto con el tokenizer utilizado.

---

## ðŸ“„ Licencia

Uso acadÃ©mico / TFG.
Si se publica abiertamente, se recomienda aÃ±adir una licencia (MIT o Apache-2.0).

```

---

### Siguiente paso
Para seguir â€œpaso a pasoâ€ como quieres:

1) dime el **nombre exacto del repo** (ej: `code2-t5`)  
2) dime si vas a subir **pesos** al repo o solo enlaces a Hugging Face

y te preparo el `requirements.txt` compatible (evitando el error de `huggingface-hub==1.2.4`) y los textos cortos del `data/README.md`.
```
